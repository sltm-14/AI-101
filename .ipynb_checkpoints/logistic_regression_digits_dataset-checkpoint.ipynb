{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"logo.png\" width=\"200\"></center> \n",
    "\n",
    "<center> <h1>Logistic Regression with Gradient Ascent</h1> </center>\n",
    "<center> <h1>Handwritten Digits Classification</h1> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Ascent algorithm\n",
    "From our [video](https://www.youtube.com/watch?v=TM1lijyQnaI&t=810s) on logistic regression, we derived the equation to update the logistic regression model parameters as:    \n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta^{+} = \\theta^{-} + \\alpha (y_{i} - h(x_{i}) )\\bar{x}\n",
    "\\end{equation}\n",
    "\n",
    "This maximizes the following log likelihood function\n",
    "\n",
    "\\begin{equation}\n",
    "J(x, \\theta, y) = \\sum_{i=1}^{m}y_i\\log(h(x_{i})) + (1 - y_i)\\log(1 - h(x_{i}))\n",
    "\\end{equation}\n",
    "\n",
    "where our hypothesis is a sigmoid function\n",
    "\\begin{equation}\n",
    "h(x_i) = \\frac{1}{1 + e^{\\theta^T \\bar{x}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient Ascent\n",
    "```FOR j FROM 0 -> max_iteration: \n",
    "    FOR i FROM 0 -> m: \n",
    "        theta += (alpha) * (y[i] - h(x[i])) * x_bar\n",
    "    ENDLOOP\n",
    "ENDLOOP\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classificaton with one-vs-all (one-vs-rest)\n",
    "If you have n-classes, we train n-classifiers and given a new data point we predict using all the classifiers and choose the one with the highest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "f, axarr = plt.subplots(3,3)\n",
    "axarr[0,0].imshow(digits.images[0])\n",
    "axarr[0,1].imshow(digits.images[1])\n",
    "axarr[0,2].imshow(digits.images[2])\n",
    "axarr[1,0].imshow(digits.images[3])\n",
    "axarr[1,1].imshow(digits.images[4])\n",
    "axarr[1,2].imshow(digits.images[5])\n",
    "axarr[2,0].imshow(digits.images[6])\n",
    "axarr[2,1].imshow(digits.images[7])\n",
    "axarr[2,2].imshow(digits.images[8])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression():\n",
    "    \"\"\"Class for training and using a model for logistic regression\"\"\"\n",
    "    \n",
    "    def set_values(self, initial_params, alpha=0.01, max_iter=5000, class_of_interest=0):\n",
    "        \"\"\"Set the values for initial params, step size, maximum iteration, and class of interest\"\"\"\n",
    "        self.params = initial_params\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.class_of_interest = class_of_interest\n",
    "    \n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        \"\"\"Sigmoide function\"\"\"\n",
    "        \n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "    \n",
    "    def predict(self, x_bar, params):\n",
    "        \"\"\"predict the probability of a class\"\"\"  \n",
    "                \n",
    "        return self._sigmoid(np.dot(params, x_bar))\n",
    "    \n",
    "    def _compute_cost(self, input_var, output_var, params):\n",
    "        \"\"\"Compute the log likelihood cost\"\"\"\n",
    "        \n",
    "        cost = 0\n",
    "        for x, y in zip(input_var, output_var):\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, params)\n",
    "            \n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            cost += y_binary * np.log(y_hat) + (1.0 - y_binary) * np.log(1 - y_hat)\n",
    "            \n",
    "        return cost\n",
    "    \n",
    "    def train(self, input_var, label, print_iter = 5000):\n",
    "        \"\"\"Train the model using batch gradient ascent\"\"\"\n",
    "        \n",
    "        iteration = 1\n",
    "        while iteration < self.max_iter:\n",
    "            if iteration % print_iter == 0:\n",
    "                print(f'iteration: {iteration}')\n",
    "                print(f'cost: {self._compute_cost(input_var, label, self.params)}')\n",
    "                print('--------------------------------------------')\n",
    "            \n",
    "            for i, xy in enumerate(zip(input_var, label)):\n",
    "                x_bar = np.array(np.insert(xy[0], 0, 1))\n",
    "                y_hat = self.predict(x_bar, self.params)\n",
    "                \n",
    "                y_binary = 1.0 if xy[1] == self.class_of_interest else 0.0\n",
    "                gradient = (y_binary - y_hat) * x_bar\n",
    "                self.params += self.alpha * gradient\n",
    "            \n",
    "            iteration +=1\n",
    "        \n",
    "        return self.params\n",
    "\n",
    "    def test(self, input_test, label_test):\n",
    "        \"\"\"Test the accuracy of the model using test data\"\"\"\n",
    "        self.total_classifications = 0\n",
    "        self.correct_classifications = 0\n",
    "        \n",
    "        for x,y in zip(input_test, label_test):\n",
    "            self.total_classifications += 1\n",
    "            x_bar = np.array(np.insert(x, 0, 1))\n",
    "            y_hat = self.predict(x_bar, self.params)\n",
    "            y_binary = 1.0 if y == self.class_of_interest else 0.0\n",
    "            \n",
    "            if y_hat >= 0.5 and  y_binary == 1:\n",
    "                # correct classification of class_of_interest\n",
    "                self.correct_classifications += 1\n",
    "              \n",
    "            if y_hat < 0.5 and  y_binary != 1:\n",
    "                # correct classification of an other class\n",
    "                self.correct_classifications += 1\n",
    "                \n",
    "        self.accuracy = self.correct_classifications / self.total_classifications\n",
    "            \n",
    "        return self.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data to training and test sets\n",
    "digits_train, digits_test, digits_label_train, digits_label_test =\\\n",
    "train_test_split(digits.data, digits.target, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classifier for the ZERO digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
    "\n",
    "max_iter = 10000\n",
    "digits_regression_model_0 = LogisticRegression()\n",
    "digits_regression_model_0.set_values(params_0, alpha, max_iter, 0)\n",
    "\n",
    "params =\\\n",
    "digits_regression_model_0.train(digits_train / 16.0, digits_label_train, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy\n",
    "digits_accuracy = digits_regression_model_0.test(digits_test / 16.0, digits_label_test)\n",
    "print(f'Accuracy of prediciting a ZERO digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classifier for the ONE digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
    "\n",
    "max_iter = 10000\n",
    "digits_regression_model_1 = LogisticRegression()\n",
    "digits_regression_model_1.set_values(params_0, alpha, max_iter, 1)\n",
    "\n",
    "params =\\\n",
    "digits_regression_model_1.train(digits_train / 16.0, digits_label_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "digits_accuracy = digits_regression_model_1.test(digits_test / 16.0, digits_label_test)\n",
    "print(f'Accuracy of prediciting a ONE digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classifier for the TWO digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
    "\n",
    "max_iter = 10000\n",
    "digits_regression_model_2 = LogisticRegression()\n",
    "digits_regression_model_2.set_values(params_0, alpha, max_iter, 2)\n",
    "\n",
    "params =\\\n",
    "digits_regression_model_2.train(digits_train / 16.0, digits_label_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_accuracy = digits_regression_model_2.test(digits_test / 16.0, digits_label_test)\n",
    "print(f'Accuracy of prediciting a TWO digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a classifier for the EIGHT digit\n",
    "alpha = 1e-2\n",
    "params_0 = np.zeros(len(digits.data[0]) + 1)\n",
    "\n",
    "max_iter = 10000\n",
    "digits_regression_model_8 = LogisticRegression()\n",
    "digits_regression_model_8.set_values(params_0, alpha, max_iter, 8)\n",
    "\n",
    "params =\\\n",
    "digits_regression_model_8.train(digits_train / 16.0, digits_label_train, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_accuracy = digits_regression_model_8.test(digits_test / 16.0, digits_label_test)\n",
    "print(f'Accuracy of prediciting a EIGHT digit in test set: {digits_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
