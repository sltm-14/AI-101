{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gVUsHr_5MurV"
   },
   "source": [
    "# Uso de spacy para desarrollo de tareas de NLP\n",
    "\n",
    "En este notebook nos enfocaremos a desarrollar algunas tareas básicas de NLP como _Tokenization_, _Lemmatization_ y _POS tagging_ utilizando la librería de Python spaCy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yqOns3xyMpUK"
   },
   "outputs": [],
   "source": [
    "# Carga de librerías\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JKIffZicOEr2"
   },
   "source": [
    "## Carga de modelos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZPnVCtpN_ip"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting es_core_news_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_md-2.3.1/es_core_news_md-2.3.1.tar.gz (47.4 MB)\n",
      "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from es_core_news_md==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.59.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.1.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.19.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (52.0.0.post20210125)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (0.8.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\sltm-14\\anaconda3\\envs\\mlenv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->es_core_news_md==2.3.1) (4.0.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('es_core_news_md')\n"
     ]
    }
   ],
   "source": [
    "# Descarga de modelo de lenguaje\n",
    "!python -m spacy download es_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sWUPNZYPOejm"
   },
   "outputs": [],
   "source": [
    "# Cargamos datos. Antes hay que reiniciar el runtime (Runtime -> Restart Runtime) e importar spacY\n",
    "import spacy\n",
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvakl0yI-8jg"
   },
   "source": [
    "Puedes encontrar más modelos de idiomas en https://spacy.io/models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FVJGTyoDR5eJ"
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "Separa en unidades básicas para analizar el texto. Normalmente, se separa en palabras ignorando los espacios. Los signos de puntuación se tomarán en cuenta en función del contexto, el cuál depende del lenguaje.\n",
    "\n",
    "A los elementos resultantes de este proceso se le conocen como _tokens_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qRVd5FyFRaBY"
   },
   "outputs": [],
   "source": [
    "# Definimos un documento o texto a analizar\n",
    "texto = \"Este es un texto de prueba para las prácticas de procesamiento del lenguaje natural impartido por Alejandro. El objetivo es probar los tres procesos que se pueden realizar al texto.\"\n",
    "#texto = \"juan paco pedro de la mar\"\n",
    "doc = nlp(texto) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5OsTpC9tSadK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juan\n",
      "paco\n",
      "pedro\n",
      "de\n",
      "la\n",
      "mar\n"
     ]
    }
   ],
   "source": [
    "# Separamos en tokens.\n",
    "for token in doc:\n",
    "  print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XoXvU4EnTMRM"
   },
   "source": [
    "## Podemos acceder a los tokens por su posición\n",
    "print(doc[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GJ684ZP5TqOC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indice:    [0, 1, 2, 3, 4, 5]\n",
      "Texto:     ['juan', 'paco', 'pedro', 'de', 'la', 'mar']\n",
      "\n",
      "\n",
      "Son letras: [True, True, True, True, True, True]\n",
      "\n",
      "\n",
      "Es signo de puntuación: [False, False, False, False, False, False]\n",
      "\n",
      "\n",
      "Es número: [False, False, False, False, False, False]\n"
     ]
    }
   ],
   "source": [
    "# Los token de spacy tienen otras propiedades útiles para analizar su contenido\n",
    "print('Indice:   ', [token.i for token in doc]) # i nos indica la posición que tiene en el texto\n",
    "print('Texto:    ', [token.text for token in doc]) # el contenido de cada token \n",
    "print(\"\\n\")\n",
    "print('Son letras:', [token.is_alpha for token in doc]) # identifica si el token es una palabra \"alpha\", formada sólo por letras\n",
    "print(\"\\n\")\n",
    "print('Es signo de puntuación:', [token.is_punct for token in doc]) # Identifica si es signo de puntiación \n",
    "print(\"\\n\")\n",
    "print('Es número:', [token.like_num for token in doc]) # Identifica si es número, a pesar de que sea texto, si dicho texto se refiere a un número se identificará como número"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kiomckW6yoXb"
   },
   "source": [
    "## Part-of-speech tagging\n",
    "\n",
    "Es útil para saber que tipos de palabras estamos manejando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HudLc3q2yqZV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juan PROPN\n",
      "paco PROPN\n",
      "pedro PROPN\n",
      "de ADP\n",
      "la DET\n",
      "mar NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "  print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PzNEMMtbApd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#  Y qué significan estas etiquetas.\n",
    "print(spacy.explain(\"LEMMA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WKR29x3R_ctw"
   },
   "source": [
    "# Entidades propias\n",
    "\n",
    "Se refiere a identificar si las palabras son entidades propias, como nombres de personas, paises, instituciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9xHzGSoCzk2G"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juan paco pedro de la mar PER\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "  print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KWt5eFc5AIXP"
   },
   "source": [
    "# Lemmatization\n",
    "\n",
    "Este proceso es para encontrar la raíz de las palabras manteniendo el sentido dentro  del idioma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qq7Zv8KzAUdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "juan \t juan\n",
      "paco \t paco\n",
      "pedro \t pedro\n",
      "de \t de\n",
      "la \t lo\n",
      "mar \t mar\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "  print(token.text, \"\\t\", token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fYPxaWHB3_N"
   },
   "source": [
    "# Búsqueda de patrones avanzados\n",
    "\n",
    "Spacy incluye un _matcher_ para buscar expresiones o patrones en los documentos. \n",
    "\n",
    "Esto es una herramienta muy poderosa porque se buscan patrones en términos de lo que significa el texto y no sólo correspondencias de texto. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aJl8D5tODfbJ"
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VipFuDasCV61"
   },
   "outputs": [],
   "source": [
    "texto = 'El día de ayer no dormí bien y compré café por la mañana.'\n",
    "doc = nlp(texto)\n",
    "\n",
    "patron = [{\"LEMMA\": 'comprar'}, {'POS': 'NOUN'}]\n",
    "buscador = Matcher(nlp.vocab)\n",
    "buscador.add('compras', None, patron)\n",
    "\n",
    "resultado = buscador(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_F6SeFoIEGhg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11387699898349888678, 8, 10)]\n"
     ]
    }
   ],
   "source": [
    "# ¿Qué contiene resultado?\n",
    "print(resultado)\n",
    "#el primero es el número del identificador que se le dió al patrón y los otros dos valores son las posiciones del texto donde se encontró "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cXA0XLXQEMQg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compré café\n"
     ]
    }
   ],
   "source": [
    "for match_id, inicio, fin in resultado:\n",
    "  print(doc[inicio:fin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5lsvZq6Ey-V"
   },
   "outputs": [],
   "source": [
    "# Espacio para que pruebes tus ideas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'El día de ayer no dormí bien y compré café y compramos cerveza por la mañana y dormi super bien'\n",
    "doc = nlp(texto)\n",
    "\n",
    "patron_1 = [{\"LEMMA\": 'comprar'}, {'POS': 'NOUN'}]\n",
    "patron_2 = [{\"LEMMA\": 'dormir'}, {'POS': 'ADV'}]\n",
    "\n",
    "buscador.add('compras', None, patron_1)\n",
    "buscador.add('calidad_dormir', None, patron_2)\n",
    "\n",
    "resultado = buscador(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dormí bien\n",
      "compré café\n",
      "compramos cerveza\n"
     ]
    }
   ],
   "source": [
    "for match_id, inicio, fin in resultado:\n",
    "  print(doc[inicio:fin])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FGW455Pu1sT2"
   },
   "source": [
    "## Representación vectorial de palabras y similitud semántica\n",
    "\n",
    "Actualmente hay dos corrientes: \n",
    "1. Bolsas de palabras\n",
    "2. Representaciones vectoriales\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1wvynnUPy2TIuarqHnKgMj5y6GN0RIvKt)\n",
    "\n",
    "\n",
    "![alt text](https://drive.google.com/uc?id=1bkrKq9N9_M-zTNFidIAP1GuT2p1rt0t9)\n",
    "\n",
    "Word2Vec es una de las metdologías basadas en redes neuronales para encontrar representaciones vectoriales de palabras. Se basa en la idea de que palabras que comparten el mismo contexto, deben de ser palabras similares.\n",
    "\n",
    "Las más usadas son las representaciones vectoriales. Las podemos usar para calcular similitud semántica entre las palabras o documentos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Y4aMEtD1yni"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12066709274720755\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Hola, soy Alejandro\")\n",
    "doc2 = nlp('Me gusta el sol.')\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ai2WZF_SGkG9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5064237378084101\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"cumpleaños\")\n",
    "doc2 = nlp('fiesta')\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4OgLGzNTGbl6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mS8UhbjNEgNi"
   },
   "source": [
    "# RETO:\n",
    "\n",
    "Con lo que hemos revisado hasta este punto: \n",
    "1. ¿Crees que puedas hacer un chatbot?\n",
    "2. ¿Qué complicaciones observas?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOyN/B1Il6RSWWaJuckIAG1",
   "collapsed_sections": [],
   "name": "practica_spacy_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
